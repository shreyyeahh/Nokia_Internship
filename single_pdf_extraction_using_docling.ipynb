{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO4x2Q6N0RxGCDcbbPCHvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyyeahh/Nokia_Internship/blob/main/single_pdf_extraction_using_docling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTqmCn7-tBst"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#! pip install -U ipywidgets\n",
        "! pip install docling\n",
        "! pip install pdf2image\n",
        "! apt-get update && apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from pdf2image import convert_from_path\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import textwrap\n",
        "import torch\n",
        "import openai\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode"
      ],
      "metadata": {
        "id": "rzgrQXj9tI3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#!pip install sentence_transformers\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AKETba9OyRpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Sentence-Transformers for semantic embeddings\n",
        "#from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "3_BUu4kKEDhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    # Set the OpenAI API key from Colab Secrets\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"OpenAI API Key set successfully.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Warning: Could not find 'OPENAI_API_KEY' in Colab Secrets.\")\n",
        "    print(\"Please set it manually or the function will fail.\")\n",
        "    # For local development, you might set it directly, but this is not recommended for notebooks:\n",
        "    # os.environ['OPENAI_API_KEY'] = \"YOUR_SK-...\""
      ],
      "metadata": {
        "id": "ncrPP8D3yH97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_log = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "-wMnwg25t5pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_file(file_path):\n",
        "    \"\"\"\n",
        "    Display a PDF file or an image file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF or image file.\n",
        "    \"\"\"\n",
        "    # Check file extension to determine the type\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        # Convert and display PDF pages as images\n",
        "        images = convert_from_path(file_path)\n",
        "        for i, image in enumerate(images):\n",
        "            plt.figure(figsize=(16, 12))\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')  # Hide axis for a cleaner look\n",
        "            plt.show()\n",
        "    elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:\n",
        "        # Open and display the image file\n",
        "        image = Image.open(file_path)\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')  # Hide axis for a cleaner look\n",
        "        plt.show()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Please provide a PDF or image file.\")"
      ],
      "metadata": {
        "id": "_JMehl5ut_Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_with_docling(input_data_path):\n",
        "    \"\"\"\n",
        "    Extracts data from a PDF or image file using the Docling library.\n",
        "    Displays the document's content as markdown and exports any tables found in the document to CSV files.\n",
        "\n",
        "    Args:\n",
        "        input_data_path (str): The path to the input file (PDF or image).\n",
        "    \"\"\" # Initialize pipeline options with table structure analysis enabled\n",
        "    pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
        "    pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
        "\n",
        "    # Create a document converter with specified format options\n",
        "    doc_converter = DocumentConverter(\n",
        "        allowed_formats=[\n",
        "                InputFormat.PDF,\n",
        "                InputFormat.IMAGE,\n",
        "            ],  # whitelist formats, non-matching files are ignored.\n",
        "         format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "        }\n",
        "    )\n",
        "    result = doc_converter.convert(input_data_path)\n",
        "    print(result.document.export_to_markdown())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    all_tables_data = []\n",
        "    for table_ix, table in enumerate(result.document.tables):\n",
        "        table_df: pd.DataFrame = table.export_to_dataframe()\n",
        "        table_content_string = table_df.to_string()\n",
        "        all_tables_data.append({\n",
        "            \"table_number\": table_ix + 1,\n",
        "            \"table_content\": table_content_string\n",
        "        })\n",
        "    summary_df = pd.DataFrame(all_tables_data)\n",
        "    return summary_df"
      ],
      "metadata": {
        "id": "FJzjxfNfue-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer (and similar tools) includes a default tokenizer Handling of Stop Words:\n",
        "# Implicit Handling:\n",
        "# Even without explicitly removing stop words beforehand, TfidfVectorizer can implicitly reduce their impact\n",
        "def create_keyword_index(df: pd.DataFrame):\n",
        "    \"\"\"Creates and returns a TF-IDF vectorizer and matrix for keyword search.\"\"\"\n",
        "    print(\"\\nCreating TF-IDF keyword index...\")\n",
        "    corpus = df['table_content'].tolist()\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    print(\"Keyword index created successfully.\")\n",
        "    return vectorizer, tfidf_matrix"
      ],
      "metadata": {
        "id": "O-kDVXoZdKol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_top_chunks(query , vectorizer, original_df , tfidf_matrix, top_n = 1):\n",
        "    \"\"\"\n",
        "    Retrieves the top N most similar chunks for a given query.\n",
        "    \"\"\"\n",
        "    # 2. Vectorize the query using the SAME fitted vectorizer\n",
        "    query_vector = vectorizer.transform([query]) #passing query as list of strings\n",
        "    # 3. Calculating the cosine similarity between query vector and all the table chunks(here individual rows)\n",
        "    # The result is a 2D array, so we take the first (and only) row\n",
        "    cosine_sim = cosine_similarity(query_vector , tfidf_matrix).flatten()\n",
        "    # 4. Get the indices of the top N scores\n",
        "    # argsort() gives indices that would sort the array, we reverse it and take the top N\n",
        "    # [0.04 , 0.5 , 0.6 , 0.7,.....] -> sorting in descending and storing their original indices [3,2,1,0,....]\n",
        "    top_indices = cosine_sim.argsort()[::-1][:top_n]\n",
        "    #5. Retrieving the original table content using the top indices\n",
        "    results = original_df.iloc[top_indices]\n",
        "    results['similarity score'] = cosine_sim[top_indices] #adding a new column to display the cosine similarities\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "tNMoYmnujyG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_answer(query: str, retrieved_chunks_df: pd.DataFrame) -> str:\n",
        "    try:\n",
        "        client = openai.OpenAI()\n",
        "    except openai.AuthenticationError:\n",
        "        return \"OpenAI API key is not set or is invalid.\"\n",
        "\n",
        "    context = \"\\n---\\n\".join(retrieved_chunks_df['table_content'])\n",
        "\n",
        "    answer_prompt = f\"\"\"\n",
        "    Using ONLY the context provided below, give a direct and comprehensive answer to the user's question.\n",
        "    If the context does not contain the information, state that the answer is not available in the documents.\n",
        "\n",
        "    Context:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "    User's Question: {query}\n",
        "\n",
        "    Final Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        answer_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful question-answering assistant that strictly uses the provided context to answer questions.\"},\n",
        "                {\"role\": \"user\", \"content\": answer_prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        final_answer = answer_response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Failed to generate a final answer. Error: {e}\"\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "Y_x6p7cxnbdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN EXECUTION SCRIPT -------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "# 1. Define the input and output file paths\n",
        "    input_file = \"/content/MT-799012W (1).pdf\"\n",
        "    output_csv_file = \"extracted_tables_summary.csv\"\n",
        "    display_file(input_file)\n",
        "\n",
        "# 2. Call the function to get the summary DataFrame\n",
        "    extracted_tables_df = extract_data_with_docling(input_file)\n",
        "    print(f\"Successfully extracted tables and saved them to '{output_csv_file}'\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Save the DataFrame to a CSV file\n",
        "    extracted_tables_df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "    # Creating tfidf vectors\n",
        "    tfidf_vectorizer, tfidf_matrix = create_keyword_index(extracted_tables_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    while True:\n",
        "            user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
        "            if user_query.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            ### Retrieving Top Chunks ###\n",
        "            top_results = retrieve_top_chunks(user_query, tfidf_vectorizer, extracted_tables_df, tfidf_matrix)\n",
        "\n",
        "\n",
        "            print(\"\\n--- Top Retrieved Chunks ---\")\n",
        "            print(top_results[['table_number', 'table_content', 'similarity score']])\n",
        "\n",
        "            # Generating the final answer\n",
        "            final_answer = generate_final_answer(user_query, top_results)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Final Answer\")\n",
        "            print(\"=\"*50)\n",
        "            print(textwrap.fill(final_answer, width=80))\n",
        "            print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "k07kFnRyusAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}