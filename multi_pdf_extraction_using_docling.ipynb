{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyyeahh/Nokia_Internship/blob/main/multi_pdf_extraction_using_docling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNuNTT71y-uG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufok0MrRzL4X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import textwrap\n",
        "import torch\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYkQofHW0nnt"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    # Set the OpenAI API key from Colab Secrets\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"OpenAI API Key set successfully.\")\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Warning: Could not find 'OPENAI_API_KEY' in Colab Secrets.\")\n",
        "    print(\"Please set it manually or the function will fail.\")\n",
        "    # For local development, you might set it directly, but this is not recommended for notebooks:\n",
        "    # os.environ['OPENAI_API_KEY'] = \"YOUR_SK-...\"\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhRGtJQ0z1YL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#! pip install -U ipywidgets\n",
        "! pip install docling\n",
        "! pip install pdf2image\n",
        "! apt-get update && apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP9Yfvxvz5bw"
      },
      "outputs": [],
      "source": [
        "# Visualization and PDF/Image handling\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBIzpYqcz9-5"
      },
      "outputs": [],
      "source": [
        "!pip install Transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ignAd5L1z_Fs"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0pj900i0D3O"
      },
      "outputs": [],
      "source": [
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwJj7GME0HTS"
      },
      "outputs": [],
      "source": [
        "# Sentence-Transformers for semantic embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmx0mSY61csr"
      },
      "outputs": [],
      "source": [
        "def extract_content_from_single_pdf(input_data_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts content from a single PDF using Docling.\n",
        "    If tables are found, it processes them.\n",
        "    If no tables are found, it treats the entire document text as a single block.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
        "        pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE\n",
        "        doc_converter = DocumentConverter(\n",
        "            allowed_formats=[InputFormat.PDF],\n",
        "            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
        "        )\n",
        "        result = doc_converter.convert(input_data_path)\n",
        "\n",
        "        all_content_blocks = []\n",
        "\n",
        "        if not result.document.tables:\n",
        "            all_content_blocks.append({\n",
        "                \"original_table_number\": 1,\n",
        "                \"table_content\": result.document.text\n",
        "            })\n",
        "        else:\n",
        "            for table_ix, table in enumerate(result.document.tables):\n",
        "                table_df = table.export_to_dataframe()\n",
        "                table_content_string = table_df.to_string()\n",
        "                all_content_blocks.append({\n",
        "                    \"original_table_number\": table_ix + 1,\n",
        "                    \"table_content\": table_content_string\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(all_content_blocks)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error processing file {Path(input_data_path).name}: {e}\")\n",
        "        return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08oaQVXe1jJr"
      },
      "outputs": [],
      "source": [
        "def process_pdf_database(directory_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes all PDFs in a given directory to create a final DataFrame of extracted content.\n",
        "    \"\"\"\n",
        "    pdf_files = list(Path(directory_path).glob('*.pdf'))\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in directory: {directory_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_pdfs_data = []\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDFs. Starting processing...\")\n",
        "\n",
        "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "        pdf_name = pdf_path.name\n",
        "        extracted_df = extract_content_from_single_pdf(str(pdf_path))\n",
        "\n",
        "        if not extracted_df.empty:\n",
        "            extracted_df['pdf_name'] = pdf_name\n",
        "            all_pdfs_data.append(extracted_df)\n",
        "\n",
        "    if not all_pdfs_data:\n",
        "        print(\"Processing complete, but no data was successfully extracted from any PDF.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    master_df = pd.concat(all_pdfs_data, ignore_index=True)\n",
        "    final_columns = ['pdf_name', 'original_table_number', 'table_content']\n",
        "    master_df = master_df[final_columns]\n",
        "\n",
        "    return master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8ul-tNT1zXh"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(df: pd.DataFrame, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "    \"\"\"\n",
        "    Creates semantic embeddings for the text chunks using a sentence-transformer model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading sentence-transformer model: {model_name}...\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(\"Model loaded. Generating embeddings for all content blocks...\")\n",
        "\n",
        "    corpus = df['table_content'].tolist()\n",
        "    embeddings = model.encode(corpus, show_progress_bar=True, convert_to_tensor=True)\n",
        "\n",
        "    print(\"Embeddings generated successfully.\")\n",
        "    return model, embeddings.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPHfGmjl2DUV"
      },
      "outputs": [],
      "source": [
        "def search_documents_semantic(query: str, model, embeddings, original_df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Searches the documents using semantic embeddings and returns the most relevant content blocks.\n",
        "    \"\"\"\n",
        "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
        "    cosine_sims = cosine_similarity(query_embedding.cpu().numpy(), embeddings).flatten()\n",
        "\n",
        "    if len(cosine_sims) > top_n:\n",
        "        top_indices = np.argpartition(-cosine_sims, top_n)[:top_n]\n",
        "        sorted_top_indices = top_indices[np.argsort(-cosine_sims[top_indices])]\n",
        "    else:\n",
        "        sorted_top_indices = np.argsort(-cosine_sims)\n",
        "\n",
        "    results = original_df.iloc[sorted_top_indices].copy()\n",
        "    results['similarity_score'] = cosine_sims[sorted_top_indices]\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Bc4Ca42G7C"
      },
      "outputs": [],
      "source": [
        "def generate_final_answer(query:str,retrieved_chunks_df:pd.DataFrame)->str:\n",
        "   \"\"\"\n",
        "    Uses OpenAI's gpt-4o-mini model to generate a final answer from retrieved chunks.\n",
        "    \"\"\"\n",
        "   try:\n",
        "      client = openai.OpenAI()\n",
        "   except openai.AuthenticationError:\n",
        "      return \"OpenAI key not set or invalid\"\n",
        "\n",
        "   #step 1 - combine the raw chunk content into single context string\n",
        "   context = \"\\n---\\n\".join(retrieved_chunks_df['table_content'])\n",
        "\n",
        "    # --- LLM CALL 1: Summarize/Clean the context ---\n",
        "   summarization_prompt = f\"\"\"\n",
        "   Based on the following raw data chunks, synthesize the key information into a clear, factual paragraph.\n",
        "   Focus on accurately extracting facts, figures, and specifications.\n",
        "   Do not add any information that is not present in the text.\n",
        "     Raw data:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "    Clean Summary:\n",
        "    \"\"\"\n",
        "\n",
        "   print(\"\\n--- Step 4a: Generating Clean Context (with gpt-4o-mini)... ---\")\n",
        "   try:\n",
        "        summarization_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert technical assistant that summarizes raw document data into clean, readable text.\"},\n",
        "                {\"role\": \"user\", \"content\": summarization_prompt}\n",
        "            ],\n",
        "            temperature=0.0 # Low temperature for factual summarization\n",
        "        )\n",
        "        clean_context = summarization_response.choices[0].message.content\n",
        "        print(textwrap.fill(clean_context, width=80))\n",
        "   except Exception as e:\n",
        "        print(f\"An error occurred during summarization: {e}\")\n",
        "        return \"Failed to generate a clean context from the retrieved documents.\"\n",
        "\n",
        "    # --- LLM CALL 2: Generate the final answer ---\n",
        "   answer_prompt = f\"\"\"\n",
        "    Using ONLY the context provided below, give a direct and comprehensive answer to the user's question.\n",
        "    Cite the source PDF for key pieces of information if available.\n",
        "    If the context does not contain the information needed to answer the question, state that the answer is not available in the provided documents.\n",
        "\n",
        "    Context:\n",
        "    ---\n",
        "    {clean_context}\n",
        "    ---\n",
        "    User's Question: {query}\n",
        "\n",
        "    Final Answer:\n",
        "    \"\"\"\n",
        "\n",
        "   print(\"\\n--- Step 4b: Generating Final Answer (with gpt-4o-mini)... ---\")\n",
        "   try:\n",
        "        answer_response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful question-answering assistant that strictly uses the provided context to answer questions.\"},\n",
        "                {\"role\": \"user\", \"content\": answer_prompt}\n",
        "            ],\n",
        "            temperature=0.2 # Slightly higher temp for more natural language\n",
        "        )\n",
        "        final_answer = answer_response.choices[0].message.content\n",
        "   except Exception as e:\n",
        "        print(f\"An error occurred during final answer generation: {e}\")\n",
        "        return \"Failed to generate a final answer.\"\n",
        "\n",
        "   return final_answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZYcjrnK96zf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# MAIN EXECUTION SCRIPT\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. SET UP YOUR PDF DIRECTORY IN GOOGLE DRIVE\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        # --- IMPORTANT: CHANGE THIS to the path of your folder in Google Drive ---\n",
        "        PDF_DIRECTORY = '/content/drive/MyDrive/pdf_folder/'\n",
        "        print(f\"Target directory set to: {PDF_DIRECTORY}\")\n",
        "    except (ImportError, ModuleNotFoundError):\n",
        "        # Fallback for local execution if not in Colab\n",
        "        PDF_DIRECTORY = './pdf_database/'\n",
        "\n",
        "    if not os.path.exists(PDF_DIRECTORY):\n",
        "        os.makedirs(PDF_DIRECTORY)\n",
        "        print(f\"\\nCreated directory '{PDF_DIRECTORY}'.\")\n",
        "        print(\"Please upload your PDF files to this folder and run this cell again.\")\n",
        "    else:\n",
        "        # 2. CREATE THE MASTER DATAFRAME\n",
        "        # This step processes all PDFs and can take a while.\n",
        "        master_dataframe = process_pdf_database(PDF_DIRECTORY)\n",
        "\n",
        "        if not master_dataframe.empty:\n",
        "            print(f\"\\nSuccessfully created a master DataFrame with {len(master_dataframe)} text chunks.\")\n",
        "\n",
        "            # Save the master dataframe for future use so you don't have to process again\n",
        "            master_dataframe.to_csv(\"master_database.csv\", index=False)\n",
        "            print(\"\\nSaved the master DataFrame to 'master_database.csv'\")\n",
        "\n",
        "            # 3. CREATE SEMANTIC EMBEDDINGS\n",
        "            embedding_model, document_embeddings = create_embeddings(master_dataframe)\n",
        "\n",
        "            # Optional: Save embeddings for faster loading next time\n",
        "            np.save('document_embeddings.npy', document_embeddings)\n",
        "            print(\"Saved document embeddings to 'document_embeddings.npy'\")\n",
        "\n",
        "            # 4. START THE INTERACTIVE Q&A LOOP\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"System is ready. You can now ask questions.\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            user_query = input(\"Enter your semantic query (or type 'exit' to quit): \")\n",
        "            while user_query.lower() != 'exit':\n",
        "                if user_query:\n",
        "                    # Step 4a: Retrieve relevant chunks from your documents\n",
        "                    print(\"\\n--- Step 4: Retrieving relevant chunks... ---\")\n",
        "                    top_results = search_documents_semantic(\n",
        "                        user_query,\n",
        "                        embedding_model,\n",
        "                        document_embeddings,\n",
        "                        master_dataframe,\n",
        "                        top_n=5 # You can adjust this number\n",
        "                    )\n",
        "\n",
        "                    print(\"Top Retrieved Chunks (for context):\")\n",
        "                    print(top_results[['pdf_name', 'original_table_number', 'table_content']])\n",
        "\n",
        "                    # Step 4b: Pass the retrieved chunks to the LLM to generate a final answer\n",
        "                    final_answer = generate_final_answer(user_query, top_results)\n",
        "\n",
        "                    # Step 4c: Display the final, polished answer\n",
        "                    print(\"\\n\" + \"=\"*50)\n",
        "                    print(\"Final Generated Answer (GPT-4o mini)\")\n",
        "                    print(\"=\"*50)\n",
        "                    print(textwrap.fill(final_answer, width=80))\n",
        "                    print(\"=\"*50)\n",
        "\n",
        "                user_query = input(\"\\nEnter your semantic query (or type 'exit' to quit): \")\n",
        "        else:\n",
        "            print(\"\\nNo content was extracted from the PDFs. The process cannot continue.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpcBN49Nj9/BWiP2h6DfnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}